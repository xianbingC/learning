### socket编程
- 下图为tcp套接字编程流程
![socket编程](socket编程.PNG)
- 创建套接字──socket()
- 指定本地地址──bind()
- 建立连接──connect()
- 等待客户端的连接请求──accept()
- 监听连接──listen()，返回一个文件句柄，由accept监听该文件句柄
- 数据传输──read()与write()
- 关闭套接字──close()

### 同步和异步
- 同步和异步的主体是线程
- 同步是指用户线程发起IO请求后，需要等待或者轮询内核IO操作完成后继续执行
- 异步是指用户线程发起IO请求后，继续执行，当内核完成IO操作后会通知用户线程，或者调用用户线程注册的回调函数

### 阻塞和非阻塞
- 主体是指IO操作
- 阻塞是指IO操作需要彻底完成后才能返回到用户空间
- 非阻塞指IO操作被调用后立即返回给用户一个状态值，无需等待到IO操作彻底完成

### 同步阻塞IO
- 用户需要等待read将socket中的数据读取到buffer后，才继续处理接收的数据。整个IO请求的过程中，用户线程是被阻塞的，这导致用户在发起IO请求时，不能做任何事情，对CPU的资源利用率不够。

### 同步非阻塞IO
- 用户线程发起IO请求后，立即返回；但需要不断地调用read，尝试读取socket中的数据，直到读取成功后，才继续处理接收的数据。虽然用户线程每次发起IO请求后可以立即返回，但是为了等到数据，仍需要不断地轮询、重复请求，消耗了大量的CPU的资源。

### 异步阻塞IO和异步非阻塞IO（异步IO）
- 异步阻塞模型使用内核提供的select函数（多路分离函数），避免同步非阻塞模型中轮询等待的问题。
- 在IO多路复用模型中，用户线程收到通知后，自行读取数据、处理数据。而在异步IO模型中，则由内核读取数据，并放在缓冲区，用户线程收到通知后，直接使用即可。

### IO多路复用
- 通常情况下，一个用户线程只能发出一个IO请求，监听一个socket。而IO多路复用可以实现监听多个socket，处理多个IO请求

#### select
- 函数原型
```C++
int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
// param1: 能监听的文件描述符个数，最大值1024(__FD_SETSIZE)
// param2/3/4: 读事件/写事件/异常事件描述符集合
// param5: 超时时间
```
- 工作原理：
    - 从用户空间拷贝事件集合到内核空间
    - 当对应的事件就绪后，内核就会就会将事件集合拷贝会用户线程，并通知用户线程，select返回就绪事件个数
    - 用户线程遍历事件集合，进行处理
- 总结：
    - 每次调用都需要重新设置事件集合，因为其状态发生了改变
    - 调用select需要将监听事件集合拷贝到内核，当select返回后又需要拷贝回用户空间，开销较大。
    - select能够监听的事件个数有限，一般不超过1024。
- [程序示例](https://blog.csdn.net/weixin_43519984/article/details/105100661)

#### poll
- 函数原型
```C++
 int poll(struct pollfd* fds, nfds_t nfds, int timeout);
 // param1: 事件集合
 // param2: 文件描述符个数
 // param3: 超时时间，-1表示阻塞等待
 // ret: 就绪事件个数
```
- 工作原理：同select类似
- 总结：
    - poll同select相同，也需要客户端到内核以及内核到客户端的拷贝开销
    - poll突破了文件描述符的限制
    - poll同select相同，也需要遍历整个事件集合
    - poll不需要重新设定监听集合，即监听集合得到了复用
- [程序示例](https://blog.csdn.net/weixin_43519984/article/details/105106351)

#### epoll
- 设计思想
    - epoll在Linux内核中构建了一个文件系统，该文件系统采用红黑树来构建，红黑树在增加和删除上面的效率极高，因此是epoll高效的原因之一
    - epoll红黑树上采用事件异步唤醒，内核监听I/O，事件发生后内核搜索红黑树并将对应节点数据放入异步唤醒的事件队列中
- 函数原型
```C++
struct eventpoll {
　　...
　　/*红黑树的根节点，这棵树中存储着所有添加到epoll中的事件，
　　也就是这个epoll监控的事件*/
　　struct rb_root rbr;
　　/*双向链表rdllist保存着将要通过epoll_wait返回给用户的、满足条件的事件*/
　　struct list_head rdllist;
　　...
};
int epoll_create(int size);
// 创建一个epoll的句柄，size表示要监听的事件个数
//
int epoll_ctl(int epfd, int op, int fd, struct epoll_event* event);
// 执行epoll_ctl()时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回
// 不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据
// param1: epoll句柄
// param2: EPOLL_CTL_ADD/EPOLL_CTL_MOD/EPOLL_CTL_DEL，分别对应增改删除一个事件
// param3: event->events的值有：EPOLLIN/EPOLLOUT/EPOLLET 等，分别表示读/写/边缘触发模式
//
int epoll_wait(int epfd, struct epoll_event* events, int maxevents, int timeout);
// 等待事件就绪，返回的就绪事件存放在events->rdllist中
// 就绪事件从内核态拷贝到用户态，相较于select、poll而言，epoll只需要拷贝就绪事件
```
- 水平触发(LT)/边沿触发(ET)模式
    - 当设置了水平触发以后，以可读事件为例，当有数据到来并且数据在缓冲区待读。即使我这一次没有读取完数据，只要缓冲区里还有数据就会触发第二次，直到缓冲区里没数据。
    - 当设置了边沿触发以后，以可读事件为例，对“有数据到来”这件事为触发。
    - 为什么说边沿触发(ET) 的效率更高呢？
        1. 边沿触发只在数据到来的一刻才触发，很多时候服务器在接受大量数据时会先接受数据头部(水平触发在此触发第一次，边沿触发第一次)。
        2. 接着服务器通过解析头部决定要不要接这个数据。此时，如果不接受数据，水平触发需要手动清除，而边沿触发可以将清除工作交给一个定时的清除程序去做，自己立刻返回。
        3. 如果接受，两种方式都可以用while接收完整数据。
- epoll反应堆模型
    1. 创建epoll句柄
    2. 向树上添加监听描述符
    3. 调用epoll_wait等待事件就绪
    4. 有客户端连接上来，调用accept，得到描述符cfd，将其添加到红黑树上并监听读事件
    5. cfd上发生读就绪事件，调用注册的回调函数读取数据，然后将cfd摘下来重新设置监听写事件
    6. cfd上发生写就绪事件，调用注册的回调函数发送数据，然后将cfd摘下来重新设置监听读事件
- 总结：
    1. Epoll 没有最大并发连接的限制，上限是最大可以打开文件的数目，这个数字一般远大于 2048, 一般来说这个数目和系统内存关系很大 ，具体数目可以 cat /proc/sys/fs/file-max[599534] ，并且现在服务器的内存都很大，所以这个不是问题。
    2. 效率提升，epoll对于句柄事件的选择不是遍历的，是事件响应的，就是句柄上事件来就马上选择出来，不需要遍历整个句柄链表，因此效率非常高，内核将句柄用红黑树保存的，IO效率不随FD数目增加而线性下降。
    3. 内存拷贝， select让内核把 FD 消息通知给用户空间的时候使用了内存拷贝的方式，开销较大，但是Epoll 在这点上使用了共享内存的方式，这个内存拷贝也省略了。

### Q&A
- 为什么select有描述符限制？

- epoll真的没有就绪事件集合从内核到用户态的拷贝吗？

- epoll_wait是如何做到不遍历整个集合就能找到发生读写事件的描述符？

- 既然epoll_wait比select/poll更高效，那是不是所有场景下都可以使用epoll？select/poll有哪些场景可以使用
